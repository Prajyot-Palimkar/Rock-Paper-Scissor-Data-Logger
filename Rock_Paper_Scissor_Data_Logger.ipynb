{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5981a2",
   "metadata": {},
   "source": [
    "# Rock Paper Scissor Data Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f2216",
   "metadata": {},
   "source": [
    "### Installing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f526198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffb922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3714b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense,MaxPool2D,Dropout,Flatten,Conv2D,GlobalAveragePooling2D,Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e83ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9299e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice,shuffle\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0642f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb63cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b5fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ddd237c",
   "metadata": {},
   "source": [
    "### Gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55596b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(num_samples):\n",
    "    \n",
    "    global rock, paper, scissor, nothing\n",
    "    \n",
    "    # Initialize the camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # trigger tells us when to start recording\n",
    "    trigger = False\n",
    "    \n",
    "    # Counter keeps count of the number of samples collected\n",
    "    counter = 0\n",
    "    \n",
    "    # This the ROI size, the size of images saved will be box_size -10\n",
    "    box_size = 234\n",
    "    \n",
    "    # Getting the width of the frame from the camera properties\n",
    "    width = int(cap.get(3))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        # Read frame by frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Flip the frame laterally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Break the loop if there is trouble reading the frame.\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # If counter is equal to the number samples then reset triger and the counter\n",
    "        if counter == num_samples:\n",
    "            trigger = not trigger\n",
    "            counter = 0\n",
    "        \n",
    "        # Define ROI for capturing samples\n",
    "        cv2.rectangle(frame, (width - box_size, 0), (width, box_size), (0, 250, 150), 2)\n",
    "        \n",
    "        # Make a resizable window.\n",
    "        cv2.namedWindow(\"Collecting images\", cv2.WINDOW_NORMAL)\n",
    "        \n",
    "        \n",
    "        # If trigger is True than start capturing the samples\n",
    "        if trigger:\n",
    "            \n",
    "            # Grab only slected roi\n",
    "            roi = frame[5: box_size-5 , width-box_size + 5: width -5]\n",
    "            \n",
    "            # Append the roi and class name to the list with the selected class_name\n",
    "            eval(class_name).append([roi, class_name])\n",
    "                                    \n",
    "            # Increment the counter \n",
    "            counter += 1 \n",
    "        \n",
    "            # Text for the counter\n",
    "            text = \"Collected Samples of {}: {}\".format(class_name, counter)\n",
    "            \n",
    "        else:\n",
    "            text = \"Press 'r' to collect rock samples, 'p' for paper, 's' for scissor and 'n' for nothing\"\n",
    "        \n",
    "        # Show the counter on the imaege\n",
    "        cv2.putText(frame, text, (3, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the window\n",
    "        cv2.imshow(\"Collecting images\", frame)\n",
    "        \n",
    "        # Wait 1 ms\n",
    "        k = cv2.waitKey(1)\n",
    "        \n",
    "        # If user press 'r' than set the path for rock directoryq\n",
    "        if k == ord('r'):\n",
    "            \n",
    "            # Trigger the variable inorder to capture the samples\n",
    "            trigger = not trigger\n",
    "            class_name = 'rock'\n",
    "            rock = []\n",
    "           \n",
    "            \n",
    "        # If user press 'p' then class_name is set to paper and trigger set to True  \n",
    "        if k == ord('p'):\n",
    "            trigger = not trigger\n",
    "            class_name = 'paper'\n",
    "            paper = []\n",
    "        \n",
    "        # If user press 's' then class_name is set to scissor and trigger set to True  \n",
    "        if k == ord('s'):\n",
    "            trigger = not trigger\n",
    "            class_name = 'scissor'\n",
    "            scissor = []\n",
    "                    \n",
    "        # If user press 's' then class_name is set to nothing and trigger set to True\n",
    "        if k == ord('n'):\n",
    "            trigger = not trigger\n",
    "            class_name = 'nothing'\n",
    "            nothing = []\n",
    "        \n",
    "        # Exit if user presses 'q'\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "            \n",
    "    #  Release the camera and destroy the window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053cd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_samples = 100\n",
    "gather_data(no_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdc450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc3372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "549db7a4",
   "metadata": {},
   "source": [
    "### Visualize the dataset (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb38e8a3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11724/2660591149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Iterate for each class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mclass_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meach_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscissor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnothing\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Get 8 random indexes, since we will be showing 8 examples of each class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rock' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=[30,20])\n",
    "\n",
    "# Set the rows and columns\n",
    "rows, cols = 4, 8\n",
    "\n",
    "# Iterate for each class\n",
    "for class_index, each_list in enumerate([rock, paper, scissor,nothing]):\n",
    "    \n",
    "    # Get 8 random indexes, since we will be showing 8 examples of each class.\n",
    "    r = np.random.randint(no_of_samples, size=8);\n",
    "    \n",
    "    # Plot the examples\n",
    "    for i, example_index in enumerate(r,1):\n",
    "        plt.subplot(rows,cols,class_index*cols + i );plt.imshow(each_list[example_index][0][:,:,::-1]);plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e839dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e8f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "133d89fc",
   "metadata": {},
   "source": [
    "### Test-Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e30a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the labels of all classes together\n",
    "labels = [tupl[1] for tupl in rock] + [tupl[1] for tupl in paper] + [tupl[1] for tupl in scissor] +[tupl[1] for tupl in nothing]\n",
    "\n",
    "# Combine the images of all classes together\n",
    "images = [tupl[0] for tupl in rock] + [tupl[0] for tupl in paper] + [tupl[0] for tupl in scissor] +[tupl[0] for tupl in nothing]\n",
    "\n",
    "# Normalize the images by dividing by 255, now our images are in range 0-1. This will help in training.\n",
    "images = np.array(images, dtype=\"float\") / 255.0\n",
    "\n",
    "# Print out the total number of labels and images.\n",
    "print('Total images: {} , Total Labels: {}'.format(len(labels), len(images)))\n",
    "\n",
    "# Create an encoder Object\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Convert Lablels to integers. i.e. nothing = 0, paper = 1, rock = 2, scissor = 3 (mapping is done in alphabatical order)\n",
    "Int_labels = encoder.fit_transform(labels)\n",
    "\n",
    "# Now the convert the integer labels into one hot format. i.e. 0 = [1,0,0,0]  etc.\n",
    "one_hot_labels = to_categorical(Int_labels, 4)\n",
    "\n",
    "# Now we're splitting the data, 75% for training and 25% for testing.\n",
    "(trainX, testX, trainY, testY) = train_test_split(images, one_hot_labels, test_size=0.25, random_state=50)\n",
    "\n",
    "# Empty memory from RAM\n",
    "images = []\n",
    "\n",
    "\n",
    "# This can further free up memory from RAM but be careful, if you won't be able to change split % after this.\n",
    "# rock, paper, scissor = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebffa5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57ee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9317f0e",
   "metadata": {},
   "source": [
    "### Prepare model for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the input size which our model accepts.\n",
    "image_size = 224\n",
    "\n",
    "# Loading pre-trained NASNETMobile Model without the head by doing include_top = False\n",
    "N_mobile = tf.keras.applications.NASNetMobile( input_shape=(image_size, image_size, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze the whole model \n",
    "N_mobile.trainable = False\n",
    "    \n",
    "# Adding our own custom head\n",
    "# Start by taking the output feature maps from NASNETMobile\n",
    "x = N_mobile.output\n",
    "\n",
    "# Convert to a single-dimensional vector by Global Average Pooling. \n",
    "# We could also use Flatten()(x) GAP is more effective reduces params and controls overfitting.\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Adding a dense layer with 712 units\n",
    "x = Dense(712, activation='relu')(x) \n",
    "\n",
    "# Dropout 40% of the activations, helps reduces overfitting\n",
    "x = Dropout(0.40)(x)\n",
    "\n",
    "# The fianl layer will contain 4 output units (no of units = no of classes) with softmax function.\n",
    "preds = Dense(4,activation='softmax')(x) \n",
    "\n",
    "# Construct the full model\n",
    "model = Model(inputs=N_mobile.input, outputs=preds)\n",
    "\n",
    "# Check the number of layers in the final Model\n",
    "print (\"Number of Layers in Model: {}\".format(len(model.layers[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5112d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562aa4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eede8e1c",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding transformations that I know would help, you can feel free to add more.\n",
    "# I'm doing horizontal_flip = False, in case you aren't sure which hand you would be using you can make that True.\n",
    "\n",
    "augment = ImageDataGenerator( \n",
    "    \n",
    "        rotation_range=30,\n",
    "        zoom_range=0.25,\n",
    "        width_shift_range=0.10,\n",
    "        height_shift_range=0.10,\n",
    "        shear_range=0.10,\n",
    "        horizontal_flip=False,\n",
    "        fill_mode=\"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0583c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batchsize according to your system\n",
    "epochs = 15\n",
    "batchsize = 20\n",
    "\n",
    "# Start training\n",
    "history = model.fit(x=augment.flow(trainX, trainY, batch_size=batchsize), validation_data=(testX, testY), \n",
    "steps_per_epoch= len(trainX) // batchsize, epochs=epochs)\n",
    "\n",
    "# Use model.fit_generator function instead if TF version < 2.2\n",
    "#history = model.fit_generator(x = augment.flow(trainX, trainY, batch_size=batchsize), validation_data=(testX, testY), \n",
    "#steps_per_epoch= len(trainX) // batchsize, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d9b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db00208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be7df91c",
   "metadata": {},
   "source": [
    "### Accuracy check and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827916d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e90417",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"rps4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc70bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544330b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31c26d65",
   "metadata": {},
   "source": [
    "## Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb457c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"rps4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac666b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c2b2821",
   "metadata": {},
   "source": [
    "### Data Logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ff9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "box_size = 234\n",
    "width = int(cap.get(3))\n",
    "\n",
    "# Specify the number of attempts done intially, i.e. 5.\n",
    "rock_attempts = 0\n",
    "paper_attempts = 0\n",
    "scissor_attempts = 0\n",
    "\n",
    "# Initially the moves will be 'nothing'\n",
    "final_user_move = \"nothing\"\n",
    "\n",
    "label_names = ['nothing', 'paper', 'rock', 'scissor']\n",
    "\n",
    "\n",
    "# The default color of bounding box is Blue\n",
    "rect_color = (255, 0, 0)\n",
    "\n",
    "# This variable remembers if the hand is inside the box or not.\n",
    "hand_inside = False\n",
    "\n",
    "# At each iteration we will increase the total_attempts of each signal value by 1\n",
    "rock_total_attempts = rock_attempts\n",
    "paper_total_attempts = paper_attempts\n",
    "scissor_total_attempts = scissor_attempts\n",
    "\n",
    "# We will only consider predictions having confidence above this threshold.\n",
    "confidence_threshold = 0.70\n",
    "\n",
    "# Instead of working on a single prediction, we will take the mode of 5 predictions by using a deque object\n",
    "# This way even if we face a false positive, we would easily ignore it\n",
    "smooth_factor = 5\n",
    "\n",
    "# Our initial deque list will have 'nothing' repeated 5 times.\n",
    "de = deque(['nothing'] * 5, maxlen=smooth_factor)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "           \n",
    "    cv2.namedWindow(\"Rock Paper Scissors\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # extract the region of image within the user rectangle\n",
    "    roi = frame[5: box_size-5 , width-box_size + 5: width -5]\n",
    "    \n",
    "    roi = np.array([roi]).astype('float64') / 255.0\n",
    "    \n",
    "    # Predict the move made\n",
    "    pred = model.predict(roi)\n",
    "    \n",
    "    # Get the index of the predicted class\n",
    "    move_code = np.argmax(pred[0])\n",
    "   \n",
    "    # Get the class name of the predicted class\n",
    "    user_move = label_names[move_code]\n",
    "    \n",
    "    # Get the confidence of the predicted class\n",
    "    prob = np.max(pred[0])\n",
    "    \n",
    "    # Make sure the probability is above our defined threshold\n",
    "    if prob >= confidence_threshold:\n",
    "        \n",
    "        # Now add the move to deque list from left\n",
    "        de.appendleft(user_move)\n",
    "        \n",
    "        # Get the mode i.e. which class has occured more frequently in the last 5 moves.\n",
    "        try:\n",
    "            final_user_move = st.mode(de)[0][0] \n",
    "            \n",
    "        except StatisticsError:\n",
    "            print('Stats error')\n",
    "            continue\n",
    "             \n",
    "        # If nothing is not true and hand_inside is False then proceed.\n",
    "        # Basically the hand_inside variable is helping us to not repeatedly predict during the loop\n",
    "        # So now the user has to take his hands out of the box for every new prediction.\n",
    "        \n",
    "        if final_user_move != \"nothing\" and hand_inside == False:\n",
    "            \n",
    "            # Set hand inside to True\n",
    "            hand_inside = True \n",
    "            \n",
    "            # Add one attempt\n",
    "            if final_user_move == \"rock\":\n",
    "                rock_total_attempts += 1\n",
    "            elif final_user_move == \"paper\":\n",
    "                paper_total_attempts += 1\n",
    "            elif final_user_move == \"scissor\":\n",
    "                scissor_total_attempts += 1\n",
    "            \n",
    "        # If class is nothing then hand_inside becomes False\n",
    "        elif final_user_move == 'nothing':            \n",
    "            hand_inside = False\n",
    "            rect_color = (255, 0, 0) \n",
    "\n",
    "    # This is where all annotation is happening. \n",
    "\n",
    "    cv2.putText(frame, \"Your Move: \" + final_user_move,\n",
    "                    (190, 280), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(frame, \"Rock Attempts done: {}\".format(rock_total_attempts),\n",
    "                    (350, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 0, 255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Paper Attempts done: {}\".format(paper_total_attempts),\n",
    "                    (2, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 2, 255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"Scissor Attempts done: {}\".format(scissor_total_attempts),\n",
    "                (190, 400), cv2.FONT_HERSHEY_COMPLEX, 0.7,(100, 2, 255), 1, cv2.LINE_AA)    \n",
    "    \n",
    "    cv2.rectangle(frame, (width - box_size, 0), (width, box_size), rect_color, 2)\n",
    "    \n",
    "    with open(\"rockpaperscissor.csv\",'w',newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Rock Attempt\",\"Paper Attempt\",\"Scissor Attempt\"])\n",
    "        writer.writerow([rock_total_attempts,paper_total_attempts,scissor_total_attempts])\n",
    "\n",
    "    # Display the image    \n",
    "    cv2.imshow(\"Rock Paper Scissors\", frame)\n",
    "\n",
    "    # Exit if 'q' is pressed \n",
    "    k = cv2.waitKey(10)\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "# Relase the camera and destroy all windows.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c465e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
